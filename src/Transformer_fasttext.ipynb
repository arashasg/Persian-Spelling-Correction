{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer-fasttext.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import the transformers"
      ],
      "metadata": {
        "id": "gllxDhzaSYHa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tITid7x_o5B7",
        "outputId": "89f4aa13-fd07-411a-a377-c2ca0f869a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 24.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, TFAutoModel\n",
        "from transformers import pipeline\n",
        "\n",
        "PARSBERT = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "# config = AutoConfig.from_pretrained(PARSBERT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(PARSBERT)\n",
        "model = TFAutoModel.from_pretrained(PARSBERT)\n"
      ],
      "metadata": {
        "id": "Azu651Tgr1Z6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4e5c36-ce0a-4d05-bfc2-250fd991fe25"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import codecs\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "oNKSOgu3m_63"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI5kwKjim9Lw",
        "outputId": "17379ed5-9e14-4518-da48-6de3ceb8f9ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "normalizer = Normalizer()"
      ],
      "metadata": {
        "id": "aVNaAHf-m2rh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### normalize_input\n",
        "\n",
        "*   Input: inp: str\n",
        "*   Ouptut: normalized: str\n",
        "\n",
        "convert the *inp* string to normalized string.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ty8idW_VKj5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"پس از سال‌‌ها تلاش رازی موفق به کشف الکل شد. این دانشمند ایرانی باعث افتخار در تاریخ کور است.\"\n",
        "import string\n",
        "\n",
        "def normalize_input(inp: str):\n",
        "  inp_splitted =  inp.strip().split()\n",
        "  inp_with_halfspace = normalizer.normalize(\" \".join(inp_splitted))\n",
        "  inp_without_halfspace = inp_with_halfspace.replace(\"\\u200c\", \" \")\n",
        "  for ch in string.punctuation:\n",
        "    inp_without_halfspace = inp_without_halfspace.replace(ch, \" \"+ ch + \" \")\n",
        "  words_list = [word.strip() for word in inp_without_halfspace.split()]\n",
        "  # words_list.remove(\"ها\")\n",
        "  normalized = \" \".join(words_list)\n",
        "  return normalized\n",
        "normalize_input(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SoCH25QWmzTR",
        "outputId": "ff0165e8-b6aa-435d-b74b-f941e2b8d108"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'پس از سال ها تلاش رازی موفق به کشف الکل شد . این دانشمند ایرانی باعث افتخار در تاریخ کور است .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the ParsBert Model for masking"
      ],
      "metadata": {
        "id": "-PDMKYLEQWQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = pipeline('fill-mask', model=PARSBERT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgrY06x_s_MW",
        "outputId": "e77809a3-a86b-4a4a-8985-fd216feea3c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tester\n",
        "\n",
        "*   Input: sent: srt, ind: int\n",
        "*   Output: response: dict, masked_string: str\n",
        "\n",
        "gets a sentence and the index of the word we want to be masked. then returns a dict which looks like the proper output in the question, and a masked string.\n",
        "\n"
      ],
      "metadata": {
        "id": "SD9Z8vQvLOLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def tester(sent, ind):\n",
        "    words = sent.strip().split()\n",
        "    response = dict()\n",
        "    prev_len = sent.find(words[ind])\n",
        "    response[\"raw\"] = words[ind] \n",
        "    response[\"correct\"] = None\n",
        "    last_index = prev_len + len(words[ind]) \n",
        "    response[\"span\"] = [prev_len, last_index]\n",
        "    return response, ' '.join(words[:ind] + [\"[MASK]\"] + words[ind+1:])"
      ],
      "metadata": {
        "id": "TArK859iK7xd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### attach\n",
        "\n",
        "*   Input: toknized: list\n",
        "*   Output: string: str\n",
        "\n",
        "gets a list of tokenized words (*tokenized*) and attach each word to each other, in a way that if it has ## then remove them.\n"
      ],
      "metadata": {
        "id": "-b7RQ83dMRFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attach(tokenized):\n",
        "  string = \"\"\n",
        "  for token in tokenized:\n",
        "    if token[:2] == \"##\":\n",
        "      string += token[2:]\n",
        "    else:\n",
        "      string += \" \" + token\n",
        "  return string"
      ],
      "metadata": {
        "id": "XHZxA22dzfbS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### is_true\n",
        "\n",
        "*   Inputs: resp: dict, input_str: str, max_threshold: int\n",
        "*   Outputs:\n",
        "\n"
      ],
      "metadata": {
        "id": "QhwqeneBM-jI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_true(resp, input_str, max_threshold = 1500):\n",
        "  tokenized = tokenizer.tokenize(input_str)\n",
        "  # print(\"toknized:\", tokenized)\n",
        "  attached = attach(tokenized)\n",
        "  # print(\"attached:\", attached)\n",
        "  preds = model(attached, top_k = max_threshold)\n",
        "  # print(\"predinctoin:\", preds)\n",
        "  preds_str = [pred[\"token_str\"] for pred in preds]\n",
        "  num_of_occurence = -1\n",
        "  if resp[\"raw\"] in preds_str:\n",
        "    num_of_occurence = preds_str.index(resp[\"raw\"])\n",
        "  return resp[\"raw\"] in preds_str, num_of_occurence, preds_str\n"
      ],
      "metadata": {
        "id": "cgk9FktimrtI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone the fasttext from github then install it."
      ],
      "metadata": {
        "id": "-JjcXKu0SPOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "%cd fastText\n",
        "!pip install .\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lD5AJQOGH3i",
        "outputId": "50cd76e7-143f-4a98-95e8-326e1218e604"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3930, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 3930 (delta 29), reused 70 (delta 29), pack-reused 3854\u001b[K\n",
            "Receiving objects: 100% (3930/3930), 8.33 MiB | 22.93 MiB/s, done.\n",
            "Resolving deltas: 100% (2446/2446), done.\n",
            "/content/fastText\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/fastText\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3134352 sha256=2babc7e0331348cc7b96c17ac7fb3ac1b37087896665bb78f6109a2f5e6a02aa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gp18uz1h/wheels/22/04/6e/b3aba25c1a5845898b5871a0df37c2126cb0cc9326ad0c08e7\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.2\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import fasttext.util"
      ],
      "metadata": {
        "id": "lP3dcpFlf0P4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use the snippet below if you dont have the cc.fa.300.bin model"
      ],
      "metadata": {
        "id": "iTC5lZ_xRN3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext.util.download_model('fa', if_exists='ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "YSMHzxi9F881",
        "outputId": "793c68a0-e676-4889-80c5-cbed534db3b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cc.fa.300.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uncomment the comments if you want to access the model from the google drive\n",
        "\n",
        "It's for us, not for TAs"
      ],
      "metadata": {
        "id": "kTc-GopoRjzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvW34kuQJ6ph",
        "outputId": "5890012a-048e-4537-f77f-6a6a085c2251"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp \"/content/drive/MyDrive/Arshad/NLP/HW3-TransformersDataset/cc.fa.300.bin\" \"/content\" "
      ],
      "metadata": {
        "id": "NN4Gv6d-e4G0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To load the fasttext model, run the snippet below."
      ],
      "metadata": {
        "id": "cUjT_FzRR14L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft = fasttext.load_model('cc.fa.300.bin')"
      ],
      "metadata": {
        "id": "1M2khxiuGIvI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To copy the model to the drive uncomment the snipper below\n",
        "It's for us, not for TAs"
      ],
      "metadata": {
        "id": "jFKcdf1NR76j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp \"/content/cc.fa.300.bin\" \"/content/drive/MyDrive/Arshad/NLP/HW3-TransformersDataset\""
      ],
      "metadata": {
        "id": "lqSZ9g4_NNm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cos_sim\n",
        "\n",
        "*   Inputs: word1: str, word2: str\n",
        "*   Output: cosine similarity: float\n",
        "\n",
        "first calculate the embedding of the 2 givin words (with *fasttext*) then calcualte the cosine similarity using *scipy*\n",
        "\n"
      ],
      "metadata": {
        "id": "qLq47qnxJ72B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "def cos_sim(word1, word2):\n",
        "  emb1 = ft[word1]\n",
        "  emb2 = ft[word2]\n",
        "  return 1 - scipy.spatial.distance.cosine(emb1, emb2)"
      ],
      "metadata": {
        "id": "SWGI0PsOGLSx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaTrwZte5DyE",
        "outputId": "c0bebcd8-db7b-4d79-e282-062342587c55"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (6.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### word_sim_sent\n",
        "\n",
        "*   Inputs: sentence: str, word: str\n",
        "*   Output: Mean similarity between word and sentence\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OaXlXScT-hf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_sim_sent(sent, word):\n",
        "  words = sent.split()\n",
        "  sum_sim = 0\n",
        "  for w in words:\n",
        "    sum_sim += cos_sim(w, word)\n",
        "  return sum_sim / len(words)"
      ],
      "metadata": {
        "id": "b1pK7jqQpb5V"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### most_sim_to_sent\n",
        "\n",
        "*   Inputs: sentence: str, a list of words: list \n",
        "*   Ouput: The sorted list of similarities between each word and the sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "SGEJlWwB_Qet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def most_sim_to_sent(sent, words):\n",
        "  max_sim = 0\n",
        "  most_sim = None\n",
        "  sims = list()\n",
        "  for word in words:\n",
        "    sim = word_sim_sent(sent, word)\n",
        "    sims.append((sim, word))\n",
        "  sims.sort(reverse=True, key=lambda x: x[0])\n",
        "  return sims"
      ],
      "metadata": {
        "id": "JEBxwtMq_Ppt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### calc_sim_neighs_for_other_word\n",
        "\n",
        "*   Inputs: this_neighs: list of tuples, other_word: str\n",
        "*   Output: final_prob_neighs: list of tuples\n",
        "\n",
        "calculate similarity of each neighbor and the other word, then add new similarities to their previous probabilities as their final probability to be true.\n",
        "\n"
      ],
      "metadata": {
        "id": "p1YY5hQ7A_hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_sim_neighs_for_other_word(this_neighs, other_word):\n",
        "  final_prob_neighs = list()\n",
        "  for prob, neigh in this_neighs:\n",
        "    final_prob = prob + cos_sim(neigh, other_word)\n",
        "    final_prob_neighs.append((final_prob, neigh))\n",
        "  return final_prob_neighs"
      ],
      "metadata": {
        "id": "ZWfnW7WpA9uv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### check_sim_two_neighbors\n",
        "\n",
        "*   Inputs: prev_neighbors: list of tuples, next_neighbors: list of tuples\n",
        "*   Ouput: most_sim_neigh: str, most_prob: float\n",
        "\n",
        "check the top 3 of each of the input neighbors to see, if there is any equal neighbor, to return as the ouput.\n",
        "\n",
        "if there were more than equal neighbors, return the most probable one (by adding the probs of each in two neighbors).\n",
        "\n"
      ],
      "metadata": {
        "id": "o4L6dIRkDEfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_sim_two_neighbors(prev_neighbors, next_neighbors):\n",
        "  most_sim_neigh = None\n",
        "  most_prob = 0\n",
        "  prev_neighbors = prev_neighbors[:3]\n",
        "  next_neighbors = next_neighbors[:3]\n",
        "  if next_neighbors == None:\n",
        "    return most_sim_neigh, most_prob\n",
        "  for prev_prob, prev_neigh in prev_neighbors:\n",
        "    for next_prob, next_neigh in next_neighbors:\n",
        "      if prev_neigh == next_neigh:\n",
        "        sum_prob = prev_prob + next_prob\n",
        "        if sum_prob > most_prob:\n",
        "          most_prob = sum_prob\n",
        "          most_sim_neigh = prev_neigh\n",
        "        break\n",
        "  return most_sim_neigh, most_prob\n"
      ],
      "metadata": {
        "id": "hbYur4PgDDw6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fasttext_verification_2\n",
        "\n",
        "*   Inputs: sent: str, word: str, prev_word:str, next_word: str\n",
        "*   Output: chosen_neighbor: str, max_prob: float\n",
        "\n",
        "This verification, at first uses two adjacent words (*prev_word* & *next_word*) to find the best word instead of the given *word*.\n",
        "\n",
        "At first we use fasttext's get_nearest_neighbors method to find the top 1M similiar words to the *prev_word* (*prev_neighbors*) and then calculate the edit distances between each prev_nieghbor and the given *word*. we look for minimum edit distnace possible (except 0, if 0 break to use the min_edit_dist = 1) and we collect words which has min_edit_distances. then sort these words based on the similarity to the given sentence (*sent*) \n",
        "\n",
        "These steps will be also done for the next_word.\n",
        "\n",
        "and then best answer (according to the min_dist) will be returned from both final chosen neighbors."
      ],
      "metadata": {
        "id": "VZGKhUBXEW42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "def fasttext_verification_2(sent, word, prev_word, next_word):\n",
        "  # previous neighbors\n",
        "  prev_neighbors = ft.get_nearest_neighbors(prev_word,k=1000000)\n",
        "  prev_min_edit_dist = 100\n",
        "  prev_chosen_neighbors = list()\n",
        "  for prob, neighbor in prev_neighbors:\n",
        "    neighbor = neighbor.replace(prev_word, \"\")\n",
        "    edit_dist = gensim.similarities.fastss.editdist(neighbor, word)\n",
        "    if edit_dist < prev_min_edit_dist:\n",
        "      if edit_dist == 0:\n",
        "        break\n",
        "      prev_min_edit_dist = edit_dist\n",
        "      prev_chosen_neighbors = [(prob, neighbor)]\n",
        "      # print(\"-\"*50)\n",
        "      # print(\"new min edit distance\", prev_min_edit_dist)\n",
        "      # print(\"prev_chosen_neighbors\", prev_chosen_neighbors)\n",
        "      # print(\"-\"*50)\n",
        "    elif edit_dist == prev_min_edit_dist:\n",
        "      prev_chosen_neighbors.append((prob, neighbor))\n",
        "      # print(\"-\"*50)\n",
        "      # print(\"update prev_neighbors\", prev_chosen_neighbors)\n",
        "      # print(\"-\"*50)\n",
        "  # print(\"prev_chosen_neighbors\", prev_chosen_neighbors)\n",
        "  tmp_prev_chosen_neighs = calc_sim_neighs_for_other_word(prev_chosen_neighbors, next_word)\n",
        "  # print(\"tmp_prev_chosen_neighs\", tmp_prev_chosen_neighs)\n",
        "  final_prev_chosen_neighs = most_sim_to_sent(sent, words = [x[1] for x in tmp_prev_chosen_neighs])\n",
        "  # print(\"final_prev_chosen_neighs\", final_prev_chosen_neighs)\n",
        "\n",
        "  # next neighbors\n",
        "  next_neighbors = ft.get_nearest_neighbors(next_word,k=1000000)\n",
        "  next_min_edit_dist = 100\n",
        "  next_chosen_neighbors = list()\n",
        "  for prob, neighbor in next_neighbors:\n",
        "    neighbor = neighbor.replace(next_word, \"\")\n",
        "    edit_dist = gensim.similarities.fastss.editdist(neighbor, word)\n",
        "    if edit_dist < next_min_edit_dist:\n",
        "      if edit_dist == 0:\n",
        "        break\n",
        "      next_min_edit_dist = edit_dist\n",
        "      next_chosen_neighbors = [(prob, neighbor)]\n",
        "      # print(\"+\"*50)\n",
        "      # print(\"new min edit distance\", next_min_edit_dist)\n",
        "      # print(\"next_chosen_neighbors\", next_chosen_neighbors)\n",
        "      # print(\"+\"*50)\n",
        "    elif edit_dist == next_min_edit_dist:\n",
        "      next_chosen_neighbors.append ((prob, neighbor))\n",
        "      # print(\"+\"*50)\n",
        "      # print(\"update next_neighbors\", next_chosen_neighbors)\n",
        "      # print(\"+\"*50)\n",
        "\n",
        "  \n",
        "  tmp_next_chosen_neighs = []\n",
        "  final_next_chosen_neighs = []\n",
        "  if next_min_edit_dist <= prev_min_edit_dist:\n",
        "    # print(\"next_chosen_neighbors\", next_chosen_neighbors)\n",
        "    tmp_next_chosen_neighs = calc_sim_neighs_for_other_word(next_chosen_neighbors, prev_word)\n",
        "    # print(\"tmp_next_chosen_neighs\", tmp_next_chosen_neighs)\n",
        "    final_next_chosen_neighs = most_sim_to_sent(sent, words = [x[1] for x in tmp_next_chosen_neighs])\n",
        "    # print(\"final_next_chosen_neighs\", final_next_chosen_neighs)\n",
        "\n",
        "  if next_min_edit_dist == prev_min_edit_dist:\n",
        "      chosen_neighbor, max_prob = check_sim_two_neighbors(tmp_prev_chosen_neighs, tmp_next_chosen_neighs)\n",
        "      if chosen_neighbor != None:\n",
        "        return chosen_neighbor, max_prob \n",
        "\n",
        "\n",
        "  final_neighs = None\n",
        "  if next_min_edit_dist == prev_min_edit_dist:\n",
        "    final_neighs = final_prev_chosen_neighs + final_next_chosen_neighs\n",
        "  elif next_min_edit_dist > prev_min_edit_dist:\n",
        "    final_neighs = final_prev_chosen_neighs\n",
        "  else:\n",
        "    final_neighs = final_next_chosen_neighs\n",
        "\n",
        "  max_prob = 0\n",
        "  chosen_neighbor = None\n",
        "  for prob,neigh in final_neighs:\n",
        "    if max_prob < prob:\n",
        "      max_prob = prob\n",
        "      chosen_neighbor = neigh\n",
        "  \n",
        "  return chosen_neighbor, max_prob"
      ],
      "metadata": {
        "id": "KgaBtOuiC3Qr"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fasttext_verification\n",
        "\n",
        "*   Inputs: sent: str, word: str, near_word:str \n",
        "*   Output: chosen_neighbor: str, max_prob: float\n",
        "\n",
        "It works similiar to fasttext_verification_2, but just for one adjacent neighbor (it can be the prev_neighbor or the next_neighbor)."
      ],
      "metadata": {
        "id": "hhHLWrkaJeqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fasttext_verification(sent, word, near_word):\n",
        "  prev_neighbors = ft.get_nearest_neighbors(near_word,k=1000000)\n",
        "  min_edit_dist = 100\n",
        "  chosen_neighbors = list()\n",
        "  for prob, neighbor in prev_neighbors:\n",
        "    neighbor = neighbor.replace(near_word, \"\")\n",
        "    edit_dist = gensim.similarities.fastss.editdist(neighbor, word)\n",
        "    if edit_dist < min_edit_dist:\n",
        "      if edit_dist == 0:\n",
        "        break\n",
        "      min_edit_dist = edit_dist\n",
        "      chosen_neighbors = [(prob, neighbor)]\n",
        "      # print(\"-\"*50)\n",
        "      # print(\"new min edit distance\", min_edit_dist)\n",
        "      # print(\"chosen_neighbors\", chosen_neighbors)\n",
        "      # print(\"-\"*50)\n",
        "    elif edit_dist == min_edit_dist:\n",
        "      chosen_neighbors.append((prob, neighbor))\n",
        "      # print(\"-\"*50)\n",
        "      # print(\"update chosen_neighbors\", chosen_neighbors)\n",
        "      # print(\"-\"*50)\n",
        "  # print(\"chosen_neighbors\", chosen_neighbors)\n",
        "  final_chosen_neighs = most_sim_to_sent(sent, words = [x[1] for x in chosen_neighbors])\n",
        "  # print(\"final_chosen_neighs\", final_chosen_neighs)\n",
        "  max_prob = 0\n",
        "  chosen_neighbor = None\n",
        "  for prob,neigh in final_chosen_neighs:\n",
        "    if max_prob < prob:\n",
        "      max_prob = prob\n",
        "      chosen_neighbor = neigh\n",
        "  \n",
        "  return chosen_neighbor, max_prob"
      ],
      "metadata": {
        "id": "ZHiwJaUwt3Wn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### final_tester\n",
        "\n",
        "*   Input: text: str\n",
        "*   Output: corrected_answers: list of dicts \n",
        "\n",
        "check each word of the sentence, if the word was false, recommend the best possible word instead of that, and then as the result, returns a list of responses with the correct word.\n"
      ],
      "metadata": {
        "id": "nNHukLHDOvbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_tester(text):\n",
        "  corrected_answers = list()\n",
        "  text = normalize_input(text)\n",
        "  text_arr = text.split()\n",
        "  len_text_arr = len(text_arr)\n",
        "  for ind in range(len(text_arr)):\n",
        "    print(text)\n",
        "    resp, input_str = tester(text, ind)\n",
        "    if resp[\"raw\"] == \"ها\":\n",
        "      continue;\n",
        "    is_valid, num_of_occurence, predictions = is_true(resp, input_str, 1000)\n",
        "    if is_valid:\n",
        "      print(\"the word \", resp[\"raw\"], \" is true! and occured in number \", num_of_occurence)\n",
        "    else:\n",
        "      print(\"the word \", resp[\"raw\"], \"is false!\")\n",
        "      recom_word = None\n",
        "      if ind == 0:\n",
        "        recom_word, _ = fasttext_verification(resp[\"raw\"], text_arr[ind+1])\n",
        "      elif ind == len_text_arr - 1:\n",
        "        recom_word, _ = fasttext_verification(resp[\"raw\"], text_arr[ind-1])\n",
        "      else:\n",
        "        recom_word, _ = fasttext_verification_2(text, resp[\"raw\"], text_arr[ind-1], text_arr[ind+1])\n",
        "      resp[\"correct\"] = recom_word\n",
        "      corrected_answers.append(resp)\n",
        "      print(\" we recommed you to use the word\", recom_word)\n",
        "      text_arr[ind] = recom_word\n",
        "      text = ' '.join(text_arr)  \n",
        "    print(\"-\"*50)\n",
        "  return corrected_answers"
      ],
      "metadata": {
        "id": "MHqMkMU_jhnn"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد.\"\n",
        "final_tester(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GscSJLRG1He",
        "outputId": "0d707b55-1303-4f93-98db-b73a298ea4c7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  بسیاری  is true! and occured in number  0\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  از  is true! and occured in number  0\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  مباحث  is true! and occured in number  56\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  علوم  is true! and occured in number  0\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  غیرطبیعی  is true! and occured in number  701\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  با  is true! and occured in number  1\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  استفاده  is true! and occured in number  0\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  از  is true! and occured in number  0\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  فیزیک  is true! and occured in number  74\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  دنیای  is true! and occured in number  76\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  مادی  is true! and occured in number  45\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی ابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  ابل is false!\n",
            " we recommed you to use the word قابل\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی قابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  توجیح  is true! and occured in number  775\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی قابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  نیست  is true! and occured in number  1\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی قابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  و  is true! and occured in number  0\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی قابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  برای  is true! and occured in number  0\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی قابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  یادگیری  is true! and occured in number  63\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی قابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  باید  is true! and occured in number  0\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی قابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  به  is true! and occured in number  1\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی قابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  فلسفه  is true! and occured in number  109\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی قابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  های  is true! and occured in number  91\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی قابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  خاصی  is true! and occured in number  2\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی قابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رجو کرد .\n",
            "the word  رجو is false!\n",
            " we recommed you to use the word رو\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی قابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رو کرد .\n",
            "the word  کرد  is true! and occured in number  2\n",
            "--------------------------------------------------\n",
            "بسیاری از مباحث علوم غیرطبیعی با استفاده از فیزیک دنیای مادی قابل توجیح نیست و برای یادگیری باید به فلسفه های خاصی رو کرد .\n",
            "the word  .  is true! and occured in number  0\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'correct': 'قابل', 'raw': 'ابل', 'span': [61, 64]},\n",
              " {'correct': 'رو', 'raw': 'رجو', 'span': [115, 118]}]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"دیوار حال مستحکم نیست\"\n",
        "final_tester(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1tpGgswl8vT",
        "outputId": "b98a5e11-0cfe-487d-fe49-2b27fb6a4bfb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "دیوار حال مستحکم نیست\n",
            "the word  دیوار  is true! and occured in number  377\n",
            "--------------------------------------------------\n",
            "دیوار حال مستحکم نیست\n",
            "the word  حال is false!\n",
            " we recommed you to use the word حائل\n",
            "--------------------------------------------------\n",
            "دیوار حائل مستحکم نیست\n",
            "the word  مستحکم  is true! and occured in number  314\n",
            "--------------------------------------------------\n",
            "دیوار حائل مستحکم نیست\n",
            "the word  نیست  is true! and occured in number  62\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'correct': 'حائل', 'raw': 'حال', 'span': [6, 9]}]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"پس از سال‌‌ها تلاش رازی موفق به کسف الکل شد. این دانشمند تیرانی باعث افتخار در تاریخ کور است.\"\n",
        "final_tester(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN5XtQMyyG4_",
        "outputId": "4a5e4677-6f6e-40e4-aa35-3855947f1e46"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "پس از سال ها تلاش رازی موفق به کسف الکل شد . این دانشمند تیرانی باعث افتخار در تاریخ کور است .\n",
            "the word  پس  is true! and occured in number  0\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کسف الکل شد . این دانشمند تیرانی باعث افتخار در تاریخ کور است .\n",
            "the word  از  is true! and occured in number  0\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کسف الکل شد . این دانشمند تیرانی باعث افتخار در تاریخ کور است .\n",
            "the word  سال  is true! and occured in number  1\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کسف الکل شد . این دانشمند تیرانی باعث افتخار در تاریخ کور است .\n",
            "پس از سال ها تلاش رازی موفق به کسف الکل شد . این دانشمند تیرانی باعث افتخار در تاریخ کور است .\n",
            "the word  تلاش  is true! and occured in number  376\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کسف الکل شد . این دانشمند تیرانی باعث افتخار در تاریخ کور است .\n",
            "the word  رازی  is true! and occured in number  32\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کسف الکل شد . این دانشمند تیرانی باعث افتخار در تاریخ کور است .\n",
            "the word  موفق  is true! and occured in number  8\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کسف الکل شد . این دانشمند تیرانی باعث افتخار در تاریخ کور است .\n",
            "the word  به  is true! and occured in number  1\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کسف الکل شد . این دانشمند تیرانی باعث افتخار در تاریخ کور است .\n",
            "the word  کسف is false!\n",
            " we recommed you to use the word کشف\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کشف الکل شد . این دانشمند تیرانی باعث افتخار در تاریخ کور است .\n",
            "the word  الکل  is true! and occured in number  65\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کشف الکل شد . این دانشمند تیرانی باعث افتخار در تاریخ کور است .\n",
            "the word  شد  is true! and occured in number  0\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کشف الکل شد . این دانشمند تیرانی باعث افتخار در تاریخ کور است .\n",
            "the word  .  is true! and occured in number  0\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کشف الکل شد . این دانشمند تیرانی باعث افتخار در تاریخ کور است .\n",
            "the word  این  is true! and occured in number  0\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کشف الکل شد . این دانشمند تیرانی باعث افتخار در تاریخ کور است .\n",
            "the word  دانشمند  is true! and occured in number  16\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کشف الکل شد . این دانشمند تیرانی باعث افتخار در تاریخ کور است .\n",
            "the word  تیرانی is false!\n",
            " we recommed you to use the word ایرانی\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کشف الکل شد . این دانشمند ایرانی باعث افتخار در تاریخ کور است .\n",
            "the word  باعث  is true! and occured in number  0\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کشف الکل شد . این دانشمند ایرانی باعث افتخار در تاریخ کور است .\n",
            "the word  افتخار  is true! and occured in number  1\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کشف الکل شد . این دانشمند ایرانی باعث افتخار در تاریخ کور است .\n",
            "the word  در  is true! and occured in number  0\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کشف الکل شد . این دانشمند ایرانی باعث افتخار در تاریخ کور است .\n",
            "the word  تاریخ  is true! and occured in number  21\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کشف الکل شد . این دانشمند ایرانی باعث افتخار در تاریخ کور است .\n",
            "the word  کور is false!\n",
            " we recommed you to use the word کشور\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کشف الکل شد . این دانشمند ایرانی باعث افتخار در تاریخ کشور است .\n",
            "the word  است  is true! and occured in number  1\n",
            "--------------------------------------------------\n",
            "پس از سال ها تلاش رازی موفق به کشف الکل شد . این دانشمند ایرانی باعث افتخار در تاریخ کشور است .\n",
            "the word  .  is true! and occured in number  0\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'correct': 'کشف', 'raw': 'کسف', 'span': [31, 34]},\n",
              " {'correct': 'ایرانی', 'raw': 'تیرانی', 'span': [57, 63]},\n",
              " {'correct': 'کشور', 'raw': 'کور', 'span': [85, 88]}]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}